{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/karan/spark-2.1.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "Spark = SparkSession.builder.appName('UberCSV').getOrCreate()\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Dt', TimestampType(), True),\n",
    "                     StructField('Lat', DoubleType(), True),\n",
    "                     StructField('Lon', DoubleType(), True),\n",
    "                     StructField('Base', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/home/karan/Downloads/uber-raw-data-jul14.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dt: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Lon: string (nullable = true)\n",
      " |-- Base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "cs1 =  df.withColumn('Dt' ,unix_timestamp('Dt', 'MM/dd/yyyy HH:mm:ss').cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs2 =  cs1.withColumn(\"Lat\", df[\"Lat\"].cast(DoubleType()))\n",
    "cs =  cs2.withColumn(\"Lon\", df[\"Lon\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols = [\"Lat\", \"Lon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = assembler.transform(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.cache of DataFrame[Dt: timestamp, Lat: double, Lon: double, Base: string, features: vector]>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+------+------------------+\n",
      "|                  Dt|    Lat|     Lon|  Base|          features|\n",
      "+--------------------+-------+--------+------+------------------+\n",
      "|2014-07-01 00:03:...|40.7586|-73.9706|B02512|[40.7586,-73.9706]|\n",
      "|2014-07-01 00:05:...|40.7605|-73.9994|B02512|[40.7605,-73.9994]|\n",
      "|2014-07-01 00:06:...| 40.732|-73.9999|B02512| [40.732,-73.9999]|\n",
      "|2014-07-01 00:09:...|40.7635|-73.9793|B02512|[40.7635,-73.9793]|\n",
      "|2014-07-01 00:20:...|40.7204|-74.0047|B02512|[40.7204,-74.0047]|\n",
      "|2014-07-01 00:35:...|40.7487|-73.9869|B02512|[40.7487,-73.9869]|\n",
      "|2014-07-01 00:57:...|40.7444|-73.9961|B02512|[40.7444,-73.9961]|\n",
      "|2014-07-01 00:58:...|40.7132|-73.9492|B02512|[40.7132,-73.9492]|\n",
      "|2014-07-01 01:04:...| 40.759| -73.973|B02512|  [40.759,-73.973]|\n",
      "|2014-07-01 01:08:...|40.7601|-73.9823|B02512|[40.7601,-73.9823]|\n",
      "|2014-07-01 01:12:...|40.6951|-74.1784|B02512|[40.6951,-74.1784]|\n",
      "|2014-07-01 01:23:...|40.7203|-73.9992|B02512|[40.7203,-73.9992]|\n",
      "|2014-07-01 01:45:...|40.7575|-73.9721|B02512|[40.7575,-73.9721]|\n",
      "|2014-07-01 02:07:...|40.7471|-73.9872|B02512|[40.7471,-73.9872]|\n",
      "|2014-07-01 02:48:...|40.7808|-73.9565|B02512|[40.7808,-73.9565]|\n",
      "|2014-07-01 03:11:...|40.7624|-73.9786|B02512|[40.7624,-73.9786]|\n",
      "|2014-07-01 03:14:...| 40.815|-73.9095|B02512| [40.815,-73.9095]|\n",
      "|2014-07-01 03:20:...|40.7498|-73.9813|B02512|[40.7498,-73.9813]|\n",
      "|2014-07-01 03:28:...|40.7623|-73.9797|B02512|[40.7623,-73.9797]|\n",
      "|2014-07-01 03:38:...|40.7274|-73.9904|B02512|[40.7274,-73.9904]|\n",
      "+--------------------+-------+--------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(20).setFeaturesCol(\"features\").setPredictionCol(\"cid\").setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 40.71229703 -73.94646387]\n",
      "[ 40.7404019  -73.99366038]\n",
      "[ 40.64592574 -73.78270484]\n",
      "[ 40.71905211 -74.00141632]\n",
      "[ 40.65846337 -74.41839735]\n",
      "[ 40.22300484 -74.04608677]\n",
      "[ 40.99583714 -73.77298975]\n",
      "[ 40.75968814 -73.97900913]\n",
      "[ 40.78051135 -73.9572528 ]\n",
      "[ 40.85188784 -73.92665342]\n",
      "[ 40.80210798 -73.07387689]\n",
      "[ 40.76979444 -73.87415369]\n",
      "[ 40.75550935 -73.74543929]\n",
      "[ 40.62696132 -73.97442048]\n",
      "[ 40.71298379 -73.83936542]\n",
      "[ 40.73126006 -73.60987121]\n",
      "[ 40.68102204 -73.9800034 ]\n",
      "[ 40.74165933 -74.04418609]\n",
      "[ 40.70316743 -74.18307116]\n",
      "[ 40.78519051 -73.43029378]\n"
     ]
    }
   ],
   "source": [
    "print('Cluster Centers: ')\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansmodelsummary = model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = kmeansmodelsummary.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.createOrReplaceTempView('uber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = Spark.sql('SELECT cid, COUNT(*) AS CID_Cnt FROM uber GROUP BY cid ORDER BY 2 DESC LIMIT 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = Spark.sql('SELECT hour(uber.Dt) AS HourDT, COUNT(cid) AS Cnt FROM uber GROUP BY hour(uber.Dt) ORDER BY 2 DESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+------+------------------+---+\n",
      "|                  Dt|    Lat|     Lon|  Base|          features|cid|\n",
      "+--------------------+-------+--------+------+------------------+---+\n",
      "|2014-07-01 14:14:...|40.7186|-74.0342|B02512|[40.7186,-74.0342]| 17|\n",
      "|2014-07-01 14:14:...|40.7186|-74.0342|B02512|[40.7186,-74.0342]| 17|\n",
      "|2014-07-01 14:17:...|40.7173|-74.0354|B02512|[40.7173,-74.0354]| 17|\n",
      "|2014-07-01 16:24:...|40.7191|-74.0353|B02512|[40.7191,-74.0353]| 17|\n",
      "|2014-07-01 16:56:...|40.7223|-74.0402|B02512|[40.7223,-74.0402]| 17|\n",
      "|2014-07-01 17:27:...|40.7164|-74.0341|B02512|[40.7164,-74.0341]| 17|\n",
      "|2014-07-01 17:27:...|40.7164|-74.0341|B02512|[40.7164,-74.0341]| 17|\n",
      "|2014-07-01 18:15:...|40.7249|-74.0354|B02512|[40.7249,-74.0354]| 17|\n",
      "|2014-07-01 18:31:...|40.7513|-74.0275|B02512|[40.7513,-74.0275]| 17|\n",
      "|2014-07-01 18:49:...|40.7554|-74.0315|B02512|[40.7554,-74.0315]| 17|\n",
      "|2014-07-01 21:23:...|40.7367|-74.0312|B02512|[40.7367,-74.0312]| 17|\n",
      "|2014-07-01 21:56:...|40.7184|-74.0494|B02512|[40.7184,-74.0494]| 17|\n",
      "|2014-07-02 05:11:...|40.7839|-74.0569|B02512|[40.7839,-74.0569]| 17|\n",
      "|2014-07-02 06:36:...|40.7135| -74.035|B02512| [40.7135,-74.035]| 17|\n",
      "|2014-07-02 07:27:...|40.7495|-74.0301|B02512|[40.7495,-74.0301]| 17|\n",
      "|2014-07-02 07:27:...|40.7495|-74.0301|B02512|[40.7495,-74.0301]| 17|\n",
      "|2014-07-02 08:32:...|40.6865|-74.0722|B02512|[40.6865,-74.0722]| 17|\n",
      "|2014-07-02 08:36:...|40.7323|-74.0724|B02512|[40.7323,-74.0724]| 17|\n",
      "|2014-07-02 08:45:...|40.7147|-74.0359|B02512|[40.7147,-74.0359]| 17|\n",
      "|2014-07-02 08:45:...|40.7147|-74.0359|B02512|[40.7147,-74.0359]| 17|\n",
      "+--------------------+-------+--------+------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = Spark.sql('SELECT * FROM uber WHERE CID=17')\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
